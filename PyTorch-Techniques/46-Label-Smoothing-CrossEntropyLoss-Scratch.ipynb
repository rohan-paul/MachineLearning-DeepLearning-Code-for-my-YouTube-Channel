{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing Cross-Entropy-Loss from Scratch with PyTorch\n",
    "\n",
    "# [Link to my Youtube Video Explaining this whole Notebook](https://www.youtube.com/watch?v=PIpJn8TZJO8&list=PLxqBkZuBynVQqJTE9nRM2p7Tb12TDPlnq&index=2)\n",
    "\n",
    "[![Imgur](https://imgur.com/LoTBKcc.png)](https://www.youtube.com/watch?v=PIpJn8TZJO8&list=PLxqBkZuBynVQqJTE9nRM2p7Tb12TDPlnq&index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulae for Label Smoothing Cross Entropy loss\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/OADA4gm.png)\n",
    "\n",
    "![Imgur](https://imgur.com/PuwOVQk.png)\n",
    "\n",
    "\n",
    "Label Smoothing is designed to make the model a little bit less certain of it’s decision by changing a little bit its target:\n",
    "\n",
    "\n",
    "#### So, instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(torch.nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, \n",
    "                 reduction=\"mean\", weight=None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.epsilon   = epsilon\n",
    "        self.reduction = reduction\n",
    "        self.weight    = weight\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "         if self.reduction == 'sum' else loss\n",
    "\n",
    "    def linear_combination(self, i, j):\n",
    "        return (1 - self.epsilon) * i + self.epsilon * j\n",
    "\n",
    "    def forward(self, predict_tensor, target):\n",
    "        assert 0 <= self.epsilon < 1\n",
    "\n",
    "        if self.weight is not None:\n",
    "            self.weight = self.weight.to(predict_tensor.device)\n",
    "\n",
    "        num_classes = predict_tensor.size(-1)\n",
    "        \n",
    "        log_preds = F.log_softmax(predict_tensor, dim=-1)\n",
    "        \n",
    "        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n",
    "        \n",
    "        negative_log_likelihood_loss = F.nll_loss(\n",
    "            log_preds, target, reduction=self.reduction, weight=self.weight\n",
    "        )\n",
    "        return self.linear_combination(negative_log_likelihood_loss, loss / num_classes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4670)\n"
     ]
    }
   ],
   "source": [
    "loss_criterion = LabelSmoothingLoss(epsilon=0.5)\n",
    "\n",
    "predict_tensor = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                                [0, 0.9, 0.2, 0.2, 1], \n",
    "                                [1, 0.2, 0.7, 0.9, 1]])\n",
    "\n",
    "target = Variable(torch.LongTensor([2, 1, 0]))\n",
    "\n",
    "loss_label_smoothed = loss_criterion(Variable(predict_tensor), target )\n",
    "\n",
    "print(loss_label_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The negative log likelihood loss - `torch.nn.functional.nll_loss`\n",
    "\n",
    "\n",
    "The cross-entropy loss and the (negative) log-likelihood are\n",
    "the same in the following sense:\n",
    "\n",
    "If you apply Pytorch’s CrossEntropyLoss to your output layer,\n",
    "you get the same result as applying Pytorch’s NLLLoss to a\n",
    "LogSoftmax layer added after your original output layer.\n",
    "\n",
    "(I suspect – but don’t know for a fact – that using\n",
    "CrossEntropyLoss will be more efficient because it\n",
    "can collapse some calculations together, and doesn’t\n",
    "introduce an additional layer.)\n",
    "\n",
    "You are trying to maximize the “likelihood” of your model\n",
    "parameters (weights) having the right values. Maximizing\n",
    "the likelihood is the same as maximizing the log-likelihood,\n",
    "which is the same as minimizing the negative-log-likelihood.\n",
    "For the classification problem, the cross-entropy is the\n",
    "negative-log-likelihood. \n",
    "\n",
    "Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/pf8iEb8.png)\n",
    "\n",
    "From the above we can see that if we want to minimize the Cross-Entropy (cross-entropy loss in many deep learning libraries) we need to minimize the Negative Log-Likelihood of the model (cross-entropy loss in many libraries typically calculate Negative Log-Likelihood Loss and Log-Softmax under the hood, like in PyTorch).\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

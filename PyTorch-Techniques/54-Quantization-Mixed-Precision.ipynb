{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in PyTorch | Mixed Precision Training\n",
    "\n",
    "### [Link to my Youtube Video Explaining this whole Notebook](https://www.youtube.com/watch?v=RPvx3yZ2fc8&list=PLxqBkZuBynVQqJTE9nRM2p7Tb12TDPlnq&index=10)\n",
    "\n",
    "[![Imgur](https://imgur.com/al0WvlW.png)](https://www.youtube.com/watch?v=RPvx3yZ2fc8&list=PLxqBkZuBynVQqJTE9nRM2p7Tb12TDPlnq&index=10)\n",
    "\n",
    "\n",
    "Neural Networks are implemented as computational graphs, and their com‐\n",
    "putations often use 32-bit (or in some cases, 64-bit) floating-\n",
    "point numbers. However, we can enable our computations to use \n",
    "lower-precision numbers and still achieve comparable\n",
    "results by applying quantization.\n",
    "\n",
    "Quantization refers to techniques for computing and accessing\n",
    "memory with lower-precision data. These techniques can\n",
    "decrease model size, reduce memory bandwidth, and perform\n",
    "faster inference due to savings in memory bandwidth and\n",
    "faster computing with int8 arithmetic.\n",
    "A quick quantization method is to reduce all computation pre‐\n",
    "cision by half. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First a simple implementation of LeNet5 architecture to build a working a Neural Network on which will see the effects of Quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(\n",
    "            F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(\n",
    "            F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, \n",
    "                   int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "fp32_model = LeNet5()\n",
    "fp32_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all computations and memory are implemented as\n",
    "float32. We can inspect the data types of our model’s parame‐\n",
    "ters using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight :  torch.float32\n",
      "conv1.bias :  torch.float32\n",
      "conv2.weight :  torch.float32\n",
      "conv2.bias :  torch.float32\n",
      "fc1.weight :  torch.float32\n",
      "fc1.bias :  torch.float32\n",
      "fc2.weight :  torch.float32\n",
      "fc2.bias :  torch.float32\n",
      "fc3.weight :  torch.float32\n",
      "fc3.bias :  torch.float32\n"
     ]
    }
   ],
   "source": [
    "for n, p in fp32_model.named_parameters():\n",
    "  print(n, \": \", p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we reduce the model to half precision in one line of code using the\n",
    "half() method:\n",
    "\n",
    "Using half() is often a quick and easy way to quantize your models.\n",
    "It’s worth a try to see if the performance is adequate for your\n",
    "use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight :  torch.float16\n",
      "conv1.bias :  torch.float16\n",
      "conv2.weight :  torch.float16\n",
      "conv2.bias :  torch.float16\n",
      "fc1.weight :  torch.float16\n",
      "fc1.bias :  torch.float16\n",
      "fc2.weight :  torch.float16\n",
      "fc2.bias :  torch.float16\n",
      "fc3.weight :  torch.float16\n",
      "fc3.bias :  torch.float16\n"
     ]
    }
   ],
   "source": [
    "fp16_model = fp32_model.half()\n",
    "\n",
    "for n, p in fp16_model.named_parameters():\n",
    "  print(n, \": \", p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in many cases, we don’t want to quantize every com‐\n",
    "putation in the same way, and we may need to quantize beyond\n",
    "float16 values. For these other cases, PyTorch provides three\n",
    "additional modes of quantization: \n",
    "\n",
    "- dynamic quantization, \n",
    "\n",
    "- post-training static quantization, and \n",
    "- quantization-aware training(QAT).\n",
    "\n",
    "Dynamic quantization is used when throughput is limited by\n",
    "compute or memory bandwidth for weights. This is often true\n",
    "for LSTM, RNN or Transformer networsk.\n",
    "\n",
    "Small portions of the network – in particular, portions of the softmax operation – must remain in float32. This is because the sum of a large number of small values (our logits) can be a source of accumulated error.\n",
    "\n",
    "Static quantization is used when throughput is limited by memory band‐\n",
    "width for activations and often applies for CNNs. QAT is used\n",
    "when accuracy requirements cannot be achieved by static\n",
    "quantization.\n",
    "\n",
    "Dynamic quantization is the easiest type. It converts the activa‐\n",
    "tions to int8 on the fly. Computations use efficient int8 values,\n",
    "but the activations are read and written to memory in floating-\n",
    "point format.\n",
    "\n",
    "------------------------\n",
    "\n",
    "## Dynamic Quantization\n",
    "\n",
    "The following code shows you how to quantize a model with dynamic quantization:\n",
    "\n",
    "All we need to do is pass in our model and specify the quantized layers and the quantization level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "      fp32_model, \n",
    "      {torch.nn.Linear},\n",
    "      dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model is the PyTorch module targeted by the optimization.\n",
    "\n",
    "- {torch.nn.Linear} is the set of layer classes within the model we want to quantize.\n",
    "\n",
    "- dtype is the quantized tensor type that will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): DynamicQuantizedLinear(in_features=400, out_features=120, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): DynamicQuantizedLinear(in_features=120, out_features=84, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (fc3): DynamicQuantizedLinear(in_features=84, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model size between fp32 and int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 126.919\n",
      "model:  int8  \t Size (KB): 70.005\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(fp32_model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static quantization \n",
    "\n",
    "\n",
    "Post-training static quantization can be used to further reduce\n",
    "latency by observing the distributions of different activations\n",
    "during training and by deciding how those activations should\n",
    "be quantized at the time of inference. This type of quantization\n",
    "allows us to pass quantized values between operations without\n",
    "converting back and forth between floats and ints in memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): QuantizedConv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "  (conv2): QuantizedConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=1.0, zero_point=0)\n",
       "  (fc1): QuantizedLinear(in_features=400, out_features=120, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_quant_model = LeNet5()\n",
    "static_quant_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "torch.quantization.prepare(\n",
    "    static_quant_model, inplace=True)\n",
    "torch.quantization.convert(\n",
    "    static_quant_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization might result in reduced accuracy. In such cases, we can significantly improve the accuracy simply by using a different quantization configuration. \n",
    "\n",
    "\n",
    "### This ‘fbgemm’ configuration does the following:\n",
    "\n",
    "* Quantizes weights on a per-channel basis.\n",
    "* Uses a histogram observer that collects a histogram of activations and then picks quantization parameters in an optimal manner.\n",
    "\n",
    "\n",
    "### Prepare model for quantization\n",
    "\n",
    "`torch.quantization.prepare` will attach observers to the model. This will calibrate the training data. Calibration helps in computing the distribution of different activation. These distributions are then used to determine how activations should be quantized at inference time. Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats — and then back to ints — between every operation, resulting in a significant speed-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-aware training\n",
    "\n",
    "Quantization-aware training typically results in the best accu‐\n",
    "racy. Float values are rounded to the int8 equivalent, but the computations\n",
    "are still done in floating point. That is, the weight adjustments\n",
    "are made “aware” that they will be quantized during training.\n",
    "The following code shows how to quantize a model with QAT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qat_model = LeNet5()\n",
    "\n",
    "torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "  \n",
    "qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "torch.quantization.prepare_qat(\n",
    "    qat_model, inplace=True)\n",
    "torch.quantization.convert(\n",
    "    qat_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example to compare sizes of a model with and without Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W TensorImpl.h:1408] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63 MB\n",
      "14.26 MB\n"
     ]
    }
   ],
   "source": [
    "import torchvision \n",
    "\n",
    "model_quant = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n",
    "model_no_quant = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "\n",
    "def get_model_size(modl):\n",
    "    torch.save(modl.state_dict(), \"demo.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"demo.pt\")/1e6))\n",
    "\n",
    "# os.remove('demo.pt')\n",
    "get_model_size(model_quant)\n",
    "\n",
    "get_model_size(model_no_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a floating point model where some layers could be statically quantized\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = M()\n",
    "\n",
    "# model must be set to eval mode for static quantization logic to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<a href="https://www.youtube.com/watch?v=CwLPglxw1WA&list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&index=23"><h1 style="font-size:250%; font-family:cursive; color:#ff6666;"><b>Link YouTube Video - Microsoft Malware Detection Kaggle - 0.007 LogLoss with XGBoost </b></h1></a>

[![IMAGE ALT TEXT](https://imgur.com/HFVQ04Q.png)](https://www.youtube.com/watch?v=QI0qjDfMtAw&list=PLxqBkZuBynVS8mDTc8ZGermXiS-32pR2y&index=1)

---

### [Link to my Kaggle Notebook](https://www.kaggle.com/paulrohan2020/microsoft-malware-detection-log-loss-of-0-0070)

### [The actual Kaggle Challenge](https://www.kaggle.com/c/malware-classification/overview)

Identify whether a given piece of file/software is a malware.

### In this Notebook, I achieved a test log loss of 0.0070458 with XGBoost

(This Test Dataset is based on splitting only train.7z (which is ~200GB after extraction) into Train, Test and CV )

<font size=5 color='blue'> What is in this kernel</font>

1. [Data Description](#1)
2. [Issues I encountered for this large dataset](#2)
3. [My Final overall approach to handle this huge dataset](#3)
4. [Data Overview](#4)
5. [Performance Metric](#5)
6. [Machine Learing Objectives and Constraints](#6)
7. [Exploratory Data Analysis](#7)
8. [Distribution of malware classes in whole data set](#8)
9. [File size of byte files as a feature](#9)
10. [box plots of file size (.byte files) feature](#10)
11. [Uni-Gram Byte Feature extraction from byte files](#11)
12. [Multivariate Analysis on byte files](#12)
13. [Train Test split of only Byte Files Features](#13)
14. [Random Model ONLY on bytes files](#14)
15. [K Nearest Neighbour Classification ONLY on bytes files](#15)
16. [Logistic Regression ONLY on bytes files](#16)
17. [Random Forest Classifier ONLY on bytes files](#17)
18. [XgBoost Classification ONLY on bytes files](#18)
19. [XgBoost Classification with best hyper parameters using RandomSearch ONLY on bytes files](#19)
20. [Modeling with .asm files](#20)
21. [Feature extraction from asm files](#21)
22. [Files sizes of each .asm file as a feature](#22)
23. [Univariate analysis ONLY on .asm file features](#23)
24. [Multivariate Analysis ONLY on .asm file features](#24)
25. [Conclusion on EDA ( ONLY on .asm file features)](#25)
26. [Train and test split ( ONLY on .asm file featues )](#26)
27. [K-Nearest Neigbors ONLY on .asm file features](#27)
28. [Logistic Regression ONLY on .asm file features](#28)
29. [Random Forest Classifier ONLY on .asm file features](#29)
30. [XgBoost Classifier ONLY on .asm file features](#30)
31. [Xgboost Classifier with best hyperparameters ( ONLY on .asm file features )](#31)
32. [FINAL FEATURIZATION STEPS FOR THE FINAL XGBOOST MODEL TRAINING](#32)
33. [Uni-Gram Byte Feature extraction from byte files - For FINAL Model Train](#33)
34. [File sizes of Byte files - Feature Extraction -For FINAL Model Train](#34)
35. [Creating some important Files and Folders, which I shall use later for saving Featuarized versions of .csv files](#35)
36. [Merging Unigram of Byte Files + Size of Byte Files to create uni_gram_byte_features\_\_with_size](#36)
37. [Bi-Gram Byte Feature extraction from byte files](#37)
38. [Extracting the 2000 Most Important Features from Byte bigrams using SelectKBest with Chi-Square Test](#38)
39. [ASM Unigram - Top 52 Unigram Features from ASM Files - Final Model Training](#39)
40. [File Size of ASM Files - Feature Extraction - Final Model Training](#40)
41. [Merging ASM Unigram + ASM File Size](#41)
42. [ASM Files - Convert the ASM files to images](#42)
43. [Extract the first 800 pixel data from ASM File Images](#43)
44. [Extracting Opcodes Bigrams from ASM Files](#44)
45. [Calcualte opcodes bigram with above defined function and make them a feature and then save the data matrix of feature as a .csv file](#45)
46. [ASM File - Top Important 500 features from Opcodes Bigrams](#46)
47. [Opcodes Trigrams ASM Files - Feature extraction ](#47)
48. [SM File - Top Important 800 features from Opcodes Trigrams](#48)
49. [Final Merging of all Features for the Final XGBOOST Training](#49)
50. [Final Train Test Split. 64% Train, 16% Cross Validation, 20% Test](#50)
51. [Final XGBoost Training - Hyperparameter tuning with on Final Merged Data-Matrix](#51)
52. [Final running of XGBoost with the Best HyperParams that we got from above RandomizedSearchCV](#52)
53. [Possibiliy of Further Analysis and Featurizition](#53)

---

<h1 style="font-size:250%; font-family:cursive; color:#ff6666;"><b>1.Data Description</b><a id="1"></a></h1>

#### [Back to the top](#0)

You are provided with a set of known malware files representing a mix of 9 different families. Each malware file has an Id, a 20 character hash value uniquely identifying the file, and a Class, an integer representing one of 9 family names to which the malware may belong:

![Imgur](https://imgur.com/xyRX60l.png)

For each file, the raw data contains the hexadecimal representation of the file's binary content, without the PE header (to ensure sterility). You are also provided a metadata manifest, which is a log containing various metadata information extracted from the binary, such as function calls, strings, etc. This was generated using the IDA disassembler tool. Your task is to develop the best mechanism for classifying files in the test set into their respective family affiliations.

The dataset contains the following files:

- train.7z - the raw data for the training set (MD5 hash = 4fedb0899fc2210a6c843889a70952ed)
- trainLabels.csv - the class labels associated with the training set
- test.7z - the raw data for the test set (MD5 hash = 84b6fbfb9df3c461ed2cbbfa371ffb43)
- sampleSubmission.csv - a file showing the valid submission format
- dataSample.csv - a sample of the dataset to preview before downloading

Here we are provided with raw data and no pre-extracted features were available

#### Total train dataset consist of 200GB data out of which 50Gb of data is .bytes files and 150GB of data is .asm files:

---

<h1 style="font-size:250%; font-family:cursive; color:#ff6666;"><b> 2. Issues I encountered for this large dataset <a id="2"></a></b></h1>

#### [Back to the top](#0)

Due to the large size of the dataset (500GB), I had real issues fitting the data into memory during runtime (Colab Pro Failed, and I definitely could not run it in Kaggle)

In Kaggle I got out of disk-space when trying to extract only the train.7z file.

The below code started extracting and only at around 5% I got the out of disk error

```py
!pip install py7zr

!python -m py7zr x full_path_of_7z_file

!python -m py7zr x /content/gdrive/MyDrive/MS_Malware_Kaggle_to_Gdrive/train.7z

```

It could definitely have been done in Google-Cloud or AWS but have not tried these option.

---

<h1 style="font-size:250%; font-family:cursive; color:#ff6666;"><b>3. My Final overall approach to handle this huge dataset <a id="3"></a></b></h1>

#### [Back to the top](#0)

Therefore, I ONLY extracted train.7z (which is ~200GB after extraction) in my local machine and then split this set into Train, Test and CV set. And from this split dataset, I did my entire analysis on the train and did the validation part on test and cv set.

Further to be able to accomodate it in my local Machine (which is not too high-end )

First did all my calculations and experimentations and featuriazation ONLY on a sample of 50 files (i.e. 50 each from byteFiles and asmFile ) -

After ONLY I saw that all the featuriazation calculations and xgBoost is running on these 50 samples, then only I ran the same notebook on the full Dataset of 200GB with 20,000+ files

And here's my approach for calculating all the featuriazations (both for 50-samples and full-dataset).

1. Did all the file processing (which are CPU based) part in local machine, it took around 25 to 26 hours.

i.e. This includes calculating the below features in local machine.

- Unigram of Byte Files + Size of Byte Files + Top 52 Unigram of ASM Files (These are alrady given by AML)

Added following extra features.

- Size of ASM Files
- Top 2000 Bi-Gram of Byte files +
- Top 500 Bigram of Opcodes of ASM Files
- Top 800 Trigram of Opcodes of ASM Files
- Top 800 ASM Image Features

After merging all the above features, the merged dataframe that I got, I created to a .csv file from that (i.e. with the regular to_csv() function ).

This .csv file with the final merged dataset was just about 170MB. Then Uploaded this file to google-drive.

And then from Colab just imported that same final merged .csv file and saved that in a pandas dataframe > do train test and cv split on this and >

Now below 2 steps with Colab Pro's Tesla V100 16GB GPU

RandomizedSearchCV for hyper param tuning > and after ran XGBoost with best params.

And these final RandomizedSearchCV and XGBoost took only like 30 mints.

---

--------------

### Connect with me here..

- üê¶ TWITTER: https://twitter.com/rohanpaul_ai
- ‚Äãüë®‚Äçüîß‚Äã KAGGLE: https://www.kaggle.com/paulrohan2020
- üë®üèª‚Äçüíº LINKEDIN: https://www.linkedin.com/in/rohan-paul-b27285129/
- üë®‚Äçüíª GITHUB: https://github.com/rohan-paul
- ü§ñ Substack : https://rohanpaul.substack.com/
- üßë‚Äçü¶∞ FACEBOOK: https://www.facebook.com/rohanpaulai
- üì∏ INSTAGRAM: https://www.instagram.com/rohan_paul_2020/
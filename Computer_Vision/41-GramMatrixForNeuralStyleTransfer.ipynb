{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram matrix\n",
    "\n",
    "# [Link to my Youtube Video Explaining this whole Notebook](https://youtu.be/AIK6Gi3NUhI)\n",
    "\n",
    "[![Imgur](https://imgur.com/942zeOo.png)](https://youtu.be/AIK6Gi3NUhI)\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "In simple words, a Gram matrix (often referred to as a Gramian matrix) is a matrix created by multiplying a matrix with its own transpose.\n",
    "\n",
    "![Imgur](https://imgur.com/I6JDFEM.png)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### To Understand Gram Matrix and how they capture Styles of an Image, first we need to re-visit the concept of Dot product of 2 Vectors.\n",
    "\n",
    "And the Dot product of two vectors can be written as:\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/QDiFTT3.png)\n",
    "\n",
    "\n",
    "In the above figure, a and b are represented in a plane.\n",
    "\n",
    "Now, the more correlated a and b are, the more closer the vectors are, i.e. the more similar they are\n",
    "\n",
    "Which also means if they are closer, the angle between them, the theta will be less.\n",
    "\n",
    "And by pure Trigonometric rule The Less the Theta, the more is the cosine of theta, i.e. the more the value of the expression $|a|.|b|.cos(theta)$\n",
    "\n",
    "So ultimately, the more is the dot product between them.\n",
    "\n",
    "So with the above overall Final Rule that we get is - When the Dot Product of 2 Vectors are larger then we can infer those 2 Vectors are similar.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### But what does this theory of Vector Dot Products have to do with a Image based neural network ?\n",
    "\n",
    "And thats because, when an image moves through the various Layers of a Neural Network, the content of an image is represented by the values of the intermediate feature maps. These features maps are all Tensors, and It turns out, the style of an image can be described by the means and correlations across the different feature maps.\n",
    "\n",
    "Now for Style Extraction from your Input Images, we need to find the correlations between the features in each layer.\n",
    "\n",
    "\n",
    "The question is ……. How do I find the correlation between these features?\n",
    "\n",
    "Gram Matrix to the rescue!\n",
    "\n",
    "So here, Gram Matrix is used to determine if two matrices (in this case, filters) are correlated. It is achieved by calculating the dot-product of the vectors of the two filters. And this matrix obtained with the dot-product is called Gram Matrix.\n",
    "\n",
    "If the dot-product across the two filters is large then the two are said to be correlated and if it is small then the images are un-correlated.\n",
    "\n",
    "**********\n",
    "\n",
    "Consider two vectors(more specifically 2 flattened feature vectors from a convolutional feature map of depth C) representing features of the input space,\n",
    "\n",
    "![Imgur](https://imgur.com/gOaKrvo.png)\n",
    "\n",
    "\n",
    "Now take all C feature vectors(flattened) from this convolutional feature map of depth C and compute the dot product with every one of them(including with a feature vector itself). The result is the Gram Matrix(of size CxC).\n",
    "\n",
    "\n",
    "![Imgur](https://imgur.com/Fziq5By.png)\n",
    "\n",
    "\n",
    "This dot product of theirs gives us the information about the relation between them. The lesser the product the more different the learned features are and greater the Dot product, the more correlated the features are.\n",
    "\n",
    "In other words, the lesser the dot product, the lesser the two features co-occur and the greater the dot-product, the more they occur together.\n",
    "\n",
    "This in a sense gives information about an image’s style(texture) and zero information about its spatial structure, since we already flattened the feature and perform dot product on top of it.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Let’s see an example. Refer to below figure showing Different channels of feature maps in any particular layer. At this layer, each channel of this feature map represents the different features present in the image. Now if we can anyhow find the correlation between these features, we can get the idea of the style as correlation is nothing but the co-occurrence of the features.\n",
    "\n",
    "![Imgur](https://imgur.com/sXFCUES.png)\n",
    "\n",
    "If red channel and yellow channel are fired up with high activation values that means they have hight correlation, meaning they occur together.\n",
    "\n",
    "These two channels will have a higher correlation than that between red and green channels. We know that this co-occurrence can be calculated by calculating the correlation.\n",
    "\n",
    "This correlation of all these channels w.r.t each other is given by the Gram Matrix of an image.\n",
    "\n",
    "\n",
    "Hence, We calculate the Gram Matrix to measure the degree of correlation between channels which later will act as a measure of the style itself.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### The process of Gram matrix computation.\n",
    "\n",
    "![Imgur](https://imgur.com/TG3w0Wx.png)\n",
    "\n",
    "![Imgur](https://imgur.com/EA2d41a.png)\n",
    "\n",
    "## $$M = F * F^T$$\n",
    "\n",
    "![Imgur](https://imgur.com/chZBAZI.png)\n",
    "\n",
    "So in more simpler term, say you have a set of images and you want to calculate the Gram Metrix\n",
    "\n",
    "So to start the process, say, the images you have are of (m x n) shape.\n",
    "\n",
    "So first reshape a single one to a (m*n x 1) vector. That is Flatten the shape.\n",
    "\n",
    "Similarly convert all images to vector and then from all those Vector form a matrix ,say, M.\n",
    "\n",
    "Then the gram matrix G of these set of images will be\n",
    "\n",
    "G = M.transpose() * M;\n",
    "\n",
    "each element G(i,j) will represent the similarity measure between image i and j.\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "\n",
    "```py\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "  \"\"\" Function to  gram_matrix from image tensor \"\"\"\n",
    "\n",
    "   # Unwrap the tensor dimensions into respective variables\n",
    "   # i.e. batch size, distance, height and width\n",
    "  _,d,h,w = tensor.size()\n",
    "\n",
    "  # Flatten / Reshaping data into a two dimensional array or two dimensional of tensor\n",
    "  tensor = tensor.view(d,h*w)\n",
    "\n",
    "  # Multiplying the original tensor with its own transpose using torch.mm\n",
    "  # tensor.t() will return the transpose of original tensor\n",
    "\n",
    "  gram_matrix = torch.mm(tensor,tensor.t())\n",
    "\n",
    "  #Returning gram matrix\n",
    "  return gram_matrix\n",
    "\n",
    "```\n",
    "\n",
    "The Gram matrix computed in this technique contains dot products of the feature maps at a layer, which is a correlation operation. The entries basically encode activations that co-occur.\n",
    "\n",
    "Co-Occurrences means that texture (style) exhibits strong locality. So, when you capture activations that co-occur a lot - you capture this locality.\n",
    "```py\n",
    "\n",
    "#Initializing gram_matrix function for our tensor image\n",
    "def gram_matrix(tensor):\n",
    "   # Unwrapping the tensor dimensions into respective variables i.e. batch size, distance, height and width\n",
    "  _,d,h,w=tensor.size()\n",
    "  #Reshaping data into a two dimensional of array or two dimensional of tensor\n",
    "  tensor=tensor.view(d,h*w)\n",
    "  #Multiplying the original tensor with its own transpose using torch.mm\n",
    "  #tensor.t() will return the transpose of original tensor\n",
    "  gram=torch.mm(tensor,tensor.t())\n",
    "  #Returning gram matrix\n",
    "  return gram\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
